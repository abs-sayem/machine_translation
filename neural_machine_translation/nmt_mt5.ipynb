{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Machine Translation based on MT5 Model**\n",
    "###### **MT5 is a attention based model based on Transformer. The basic idea of this project is: we will give a input of a language (say- english)to the model and the model translate it to another language (say- bangla).**\n",
    "`I know Bangla >> Model >> আমি বাংলা জানি`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Install Dependencies**\n",
    "###### **`\"pip install transformers sentencepiece datasets\"`<br><br>Transformer: To load the model<br>Sentencepiece: To tokenize the text and <br>Dataset: We need some data to work with**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers sentencepiece datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#from google.colab import drive\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load the model**\n",
    "###### **Huggingface has various pretrained model. We can use any of these model and fine tune the model with our dataset. But we here use google multilangual mt5 small model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = 'google/mt5.small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24d7359adec4ffe2916680474ceb48a86338759ffb8252cd67d6683f84078a4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
