{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_file = \"eng-ban.txt\"\n",
    "text_file = \"ban-eng.txt\"\n",
    "# To open the text file we need to encode the text. Here, we use 'utf8' encoding\n",
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    bangla, english = line.split(\" >>> \")\n",
    "    bangla = \"[start] \" + bangla + \" [end]\"\n",
    "    text_pairs.append((english, bangla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Sentences:\", len(text_pairs))\n",
    "#text_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bangladesh is a south-asian country.', '[start] বাংলাদেশ দক্ষিন এশিয়ার একটি দেশ. [end]')\n",
      "('I know Bangla.', '[start] আমি বাংলা জানি. [end]')\n",
      "('No.', '[start] না. [end]')\n",
      "('Are you mad!', '[start] আপনি কি পাগল! [end]')\n",
      "('Bangla is my mother tounge.', '[start] বাংলা আমার মাতৃভাষা. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    print(random.choice(text_pairs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "total_data_length = len(text_pairs)\n",
    "num_val_samples = int(0.20 * total_data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(total_data_length)\n",
    "print(num_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = (total_data_length - 2 * num_val_samples)\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Pairs: 12\n",
      "Number of Val Pairs: 4\n",
      "Number of Test Pairs: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Train Pairs: {len(train_pairs)}\")\n",
    "print(f\"Number of Val Pairs: {len(val_pairs)}\")\n",
    "print(f\"Number of Test Pairs: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation + \"\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"[{re.escape(strip_chars)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return(tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\"\n",
    "    ))\n",
    "\n",
    "vocab_size = 10000\n",
    "sequence_length = 20\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length + 1,\n",
    "    standardize = custom_standardization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_bangla_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_bangla_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, ban):\n",
    "    eng = source_vectorization(eng)\n",
    "    ban = source_vectorization(ban)\n",
    "    return({\"english\": eng, \"bangla\": ban[:, :-1]}, ban[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, ban_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    ban_texts = list(ban_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ban_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls = 4)\n",
    "    return(dataset.shuffle(2048).prefetch(16).cache())\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(train_ds.as_numpy_iterator()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (12, 20)\n",
      "inputs['bangla'].shape: (12, 19)\n",
      "target.shape: (12, 19)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['bangla'].shape: {inputs['bangla'].shape}\")\n",
    "    print(f\"target.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"bangla\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 28s 28s/step - loss: 0.2064 - accuracy: 0.8209 - val_loss: 0.6601 - val_accuracy: 0.2000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7203 - accuracy: 0.1791 - val_loss: 0.3565 - val_accuracy: 0.8000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2305 - accuracy: 0.8209 - val_loss: 0.1750 - val_accuracy: 0.8000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1117 - accuracy: 0.8209 - val_loss: 0.1627 - val_accuracy: 0.8000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1107 - accuracy: 0.8358 - val_loss: 0.1760 - val_accuracy: 0.8000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1469 - accuracy: 0.8209 - val_loss: 0.1982 - val_accuracy: 0.6000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1572 - accuracy: 0.6269 - val_loss: 0.1932 - val_accuracy: 0.8000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1363 - accuracy: 0.8209 - val_loss: 0.1625 - val_accuracy: 0.8000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1051 - accuracy: 0.8060 - val_loss: 0.1611 - val_accuracy: 0.8000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0966 - accuracy: 0.8358 - val_loss: 0.1528 - val_accuracy: 0.8000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0870 - accuracy: 0.8507 - val_loss: 0.1627 - val_accuracy: 0.8000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1160 - accuracy: 0.8209 - val_loss: 0.2234 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1965 - accuracy: 0.5672 - val_loss: 0.2115 - val_accuracy: 0.8000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1568 - accuracy: 0.8209 - val_loss: 0.1547 - val_accuracy: 0.8000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0980 - accuracy: 0.8507 - val_loss: 0.1481 - val_accuracy: 0.8000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0871 - accuracy: 0.8060 - val_loss: 0.1402 - val_accuracy: 0.8000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0732 - accuracy: 0.8657 - val_loss: 0.1356 - val_accuracy: 0.9000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0771 - accuracy: 0.9254 - val_loss: 0.3183 - val_accuracy: 0.8000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2668 - accuracy: 0.8209 - val_loss: 0.1987 - val_accuracy: 0.7000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1692 - accuracy: 0.6866 - val_loss: 0.2254 - val_accuracy: 0.8000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1554 - accuracy: 0.8209 - val_loss: 0.1440 - val_accuracy: 0.8000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0923 - accuracy: 0.8358 - val_loss: 0.1473 - val_accuracy: 0.8000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0898 - accuracy: 0.8209 - val_loss: 0.1318 - val_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0774 - accuracy: 0.8806 - val_loss: 0.1483 - val_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0672 - accuracy: 0.8806 - val_loss: 0.1317 - val_accuracy: 0.9000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1011 - accuracy: 0.8060 - val_loss: 0.2448 - val_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1478 - accuracy: 0.8209 - val_loss: 0.1554 - val_accuracy: 0.8000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1260 - accuracy: 0.7463 - val_loss: 0.1838 - val_accuracy: 0.8000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1144 - accuracy: 0.8209 - val_loss: 0.1290 - val_accuracy: 0.8500\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0917 - accuracy: 0.8507 - val_loss: 0.1404 - val_accuracy: 0.8000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0754 - accuracy: 0.8657 - val_loss: 0.1191 - val_accuracy: 0.9500\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0715 - accuracy: 0.9552 - val_loss: 0.1930 - val_accuracy: 0.8000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0809 - accuracy: 0.8209 - val_loss: 0.1747 - val_accuracy: 0.7500\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1633 - accuracy: 0.6567 - val_loss: 0.2082 - val_accuracy: 0.8000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1171 - accuracy: 0.8209 - val_loss: 0.1266 - val_accuracy: 0.9000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0802 - accuracy: 0.8657 - val_loss: 0.1380 - val_accuracy: 0.8000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0740 - accuracy: 0.8657 - val_loss: 0.1125 - val_accuracy: 0.9000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0622 - accuracy: 0.9104 - val_loss: 0.2168 - val_accuracy: 0.8000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0939 - accuracy: 0.8358 - val_loss: 0.1764 - val_accuracy: 0.7500\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1545 - accuracy: 0.6866 - val_loss: 0.1904 - val_accuracy: 0.8000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1079 - accuracy: 0.8209 - val_loss: 0.1230 - val_accuracy: 0.8000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0864 - accuracy: 0.8507 - val_loss: 0.1396 - val_accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0590 - accuracy: 0.8657 - val_loss: 0.1112 - val_accuracy: 0.9500\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0686 - accuracy: 0.8955 - val_loss: 0.1766 - val_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0791 - accuracy: 0.8507 - val_loss: 0.1582 - val_accuracy: 0.8000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1384 - accuracy: 0.7463 - val_loss: 0.2015 - val_accuracy: 0.8000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0967 - accuracy: 0.8209 - val_loss: 0.1193 - val_accuracy: 0.9500\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0813 - accuracy: 0.8806 - val_loss: 0.1468 - val_accuracy: 0.8000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0640 - accuracy: 0.8358 - val_loss: 0.1079 - val_accuracy: 0.9500\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0585 - accuracy: 0.8657 - val_loss: 0.1376 - val_accuracy: 0.8000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0582 - accuracy: 0.8657 - val_loss: 0.1218 - val_accuracy: 0.9000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0868 - accuracy: 0.8358 - val_loss: 0.3000 - val_accuracy: 0.8000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1532 - accuracy: 0.8209 - val_loss: 0.1693 - val_accuracy: 0.7500\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1484 - accuracy: 0.7164 - val_loss: 0.1660 - val_accuracy: 0.8000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0898 - accuracy: 0.8358 - val_loss: 0.1225 - val_accuracy: 0.8000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0682 - accuracy: 0.9104 - val_loss: 0.1245 - val_accuracy: 0.8000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0675 - accuracy: 0.8657 - val_loss: 0.1129 - val_accuracy: 0.8000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0471 - accuracy: 0.9701 - val_loss: 0.1282 - val_accuracy: 0.8000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0505 - accuracy: 0.9104 - val_loss: 0.0988 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0628 - accuracy: 0.9104 - val_loss: 0.2407 - val_accuracy: 0.8000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0992 - accuracy: 0.8358 - val_loss: 0.1532 - val_accuracy: 0.8500\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1365 - accuracy: 0.7463 - val_loss: 0.1805 - val_accuracy: 0.8000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0923 - accuracy: 0.8209 - val_loss: 0.1075 - val_accuracy: 0.9500\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0648 - accuracy: 0.9104 - val_loss: 0.1432 - val_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0508 - accuracy: 0.9104 - val_loss: 0.1040 - val_accuracy: 0.9500\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0465 - accuracy: 0.9403 - val_loss: 0.1578 - val_accuracy: 0.8000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0538 - accuracy: 0.8806 - val_loss: 0.1032 - val_accuracy: 0.9000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0727 - accuracy: 0.8806 - val_loss: 0.2315 - val_accuracy: 0.8000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0986 - accuracy: 0.8209 - val_loss: 0.1500 - val_accuracy: 0.8500\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1326 - accuracy: 0.7612 - val_loss: 0.1807 - val_accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0756 - accuracy: 0.8209 - val_loss: 0.1032 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0538 - accuracy: 0.9403 - val_loss: 0.1281 - val_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0534 - accuracy: 0.8955 - val_loss: 0.1161 - val_accuracy: 0.8000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0477 - accuracy: 0.9552 - val_loss: 0.1066 - val_accuracy: 0.9000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0454 - accuracy: 0.9403 - val_loss: 0.1684 - val_accuracy: 0.8000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0521 - accuracy: 0.8955 - val_loss: 0.1610 - val_accuracy: 0.8500\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1553 - accuracy: 0.7761 - val_loss: 0.2519 - val_accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0887 - accuracy: 0.8358 - val_loss: 0.1153 - val_accuracy: 0.9000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0855 - accuracy: 0.8358 - val_loss: 0.1375 - val_accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0659 - accuracy: 0.8806 - val_loss: 0.1012 - val_accuracy: 0.9500\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0406 - accuracy: 0.9552 - val_loss: 0.1409 - val_accuracy: 0.8000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0450 - accuracy: 0.9104 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0532 - accuracy: 0.9254 - val_loss: 0.2579 - val_accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1125 - accuracy: 0.8358 - val_loss: 0.1609 - val_accuracy: 0.8500\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1484 - accuracy: 0.7612 - val_loss: 0.1479 - val_accuracy: 0.8000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0607 - accuracy: 0.8507 - val_loss: 0.1078 - val_accuracy: 0.9500\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0561 - accuracy: 0.9104 - val_loss: 0.1162 - val_accuracy: 0.8500\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0395 - accuracy: 0.9552 - val_loss: 0.0988 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0422 - accuracy: 0.9403 - val_loss: 0.1322 - val_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0344 - accuracy: 0.9552 - val_loss: 0.0853 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0431 - accuracy: 0.9254 - val_loss: 0.2099 - val_accuracy: 0.8000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0563 - accuracy: 0.8806 - val_loss: 0.1397 - val_accuracy: 0.8500\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1177 - accuracy: 0.8060 - val_loss: 0.2203 - val_accuracy: 0.8000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0629 - accuracy: 0.8209 - val_loss: 0.0890 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0568 - accuracy: 0.9104 - val_loss: 0.1618 - val_accuracy: 0.8000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0327 - accuracy: 0.9851 - val_loss: 0.0942 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0367 - accuracy: 0.9701 - val_loss: 0.1708 - val_accuracy: 0.8000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0337 - accuracy: 0.9403 - val_loss: 0.0820 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0655 - accuracy: 0.8806 - val_loss: 0.2798 - val_accuracy: 0.8000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0855 - accuracy: 0.8657 - val_loss: 0.1253 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a826370790>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer = \"rmsprop\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "seq2seq_rnn.fit(train_ds, epochs=100, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ban_vocab = target_vectorization.get_vocabulary()\n",
    "ban_index_lookup = dict(zip(range(len(ban_vocab)), ban_vocab))\n",
    "max_decoded_sentence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '', 1: '[UNK]', 2: '[start]', 3: '[end]', 4: 'বাংলা', 5: 'কি', 6: 'পাগল', 7: 'না', 8: 'জানি', 9: 'আমি', 10: 'আপনি', 11: 'হ্যাঁ', 12: 'হেরেছি', 13: 'হায়', 14: 'মানুষ', 15: 'মাতৃভাষা', 16: 'বাংলায়', 17: 'বাংলাদেশে', 18: 'বলে', 19: 'ফেব্রুয়ারি', 20: 'প্রায়', 21: 'নথি', 22: 'দুঃখিত', 23: 'দিবস', 24: 'থেকে', 25: 'তুমি', 26: 'গিয়েছেন', 27: 'খেলায়', 28: 'কোটি', 29: 'কথা', 30: 'কখনো', 31: 'এটা', 32: 'একুশে', 33: 'এক', 34: 'ইংরেজি', 35: 'আমরা', 36: 'আন্তর্জাতিক', 37: 'অনুবাদের'}\n"
     ]
    }
   ],
   "source": [
    "print(ban_index_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eng Text: 12\n",
      "Test Eng Text: 4\n",
      "Val Eng Text: 4\n",
      "\n",
      "Train Ban Text: 12\n",
      "Test Ban Text: 4\n",
      "Val Ban Text: 4\n"
     ]
    }
   ],
   "source": [
    "# English texts\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "print(f\"Train Eng Text: {len(train_eng_texts)}\")\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "print(f\"Test Eng Text: {len(test_eng_texts)}\")\n",
    "\n",
    "val_eng_texts = [pair[0] for pair in val_pairs]\n",
    "print(f\"Val Eng Text: {len(val_eng_texts)}\")\n",
    "\n",
    "# Bangla texts\n",
    "train_ban_texts = [pair[1] for pair in train_pairs]\n",
    "print(f\"\\nTrain Ban Text: {len(train_ban_texts)}\")\n",
    "\n",
    "test_ban_texts = [pair[1] for pair in test_pairs]\n",
    "print(f\"Test Ban Text: {len(test_ban_texts)}\")\n",
    "\n",
    "val_ban_texts = [pair[1] for pair in val_pairs]\n",
    "print(f\"Val Ban Text: {len(val_ban_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About seventy percentage of people are literate.\n"
     ]
    }
   ],
   "source": [
    "input_sentence = random.choice(test_eng_texts)\n",
    "print(input_sentence)\n",
    "tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "#print(tokenized_input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Test purpose\\ninput_sentence1 = random.choice(test_pairs)\\nprint(input_sentence1[0])\\nprint(input_sentence1[1])\\ntokenized_input_sentence10 = source_vectorization([input_sentence1[0]])\\nprint(tokenized_input_sentence0)\\ntokenized_input_sentence11 = source_vectorization([input_sentence1[1]])\\nprint(tokenized_input_sentence1)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Test purpose\n",
    "input_sentence1 = random.choice(test_pairs)\n",
    "print(input_sentence1[0])\n",
    "print(input_sentence1[1])\n",
    "tokenized_input_sentence10 = source_vectorization([input_sentence1[0]])\n",
    "print(tokenized_input_sentence0)\n",
    "tokenized_input_sentence11 = source_vectorization([input_sentence1[1]])\n",
    "print(tokenized_input_sentence1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 21), dtype=int64)\n",
      "(10000,)\n",
      "[7.4464520e-03 9.9248588e-01 7.2881812e-09 ... 6.1897896e-09 5.3716822e-09\n",
      " 1.0599247e-08]\n",
      "1\n",
      "[UNK]\n",
      "[start] [UNK]\n",
      "\n",
      "Next word prediction...\n",
      "\n",
      "tf.Tensor([[2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 21), dtype=int64)\n",
      "(10000,)\n",
      "[2.9025363e-02 9.7093630e-01 4.2610129e-09 ... 3.4295125e-09 3.2348813e-09\n",
      " 5.9960077e-09]\n",
      "1\n",
      "[UNK]\n",
      "[start] [UNK] [UNK]\n",
      "\n",
      "Next word prediction...\n",
      "\n",
      "tf.Tensor([[2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 21), dtype=int64)\n",
      "(10000,)\n",
      "[2.9025363e-02 9.7093630e-01 4.2610129e-09 ... 3.4295125e-09 3.2348813e-09\n",
      " 5.9960077e-09]\n",
      "1\n",
      "[UNK]\n",
      "[start] [UNK] [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = \"[start]\"    # The decoded sentence (bangla) that begins with \"start\"\n",
    "tokenized_target_sentence = target_vectorization([decoded_sentence])    # It will vectorize the decoded sentence\n",
    "print(tokenized_target_sentence)\n",
    "next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence]) # We use seqRNN -\n",
    "# to predict the next word. It calculates the probability of all vocabs, the highest probability holder will be the next word.\n",
    "print(next_token_predictions[0, 0, :].shape)\n",
    "print(next_token_predictions[0, 0, :])\n",
    "sampled_token_index = np.argmax(next_token_predictions[0, 0, :]);   # The index of the highest probability\n",
    "print(sampled_token_index)\n",
    "sampled_token = ban_index_lookup[sampled_token_index]   # The sample_token_index will retrive the corresponding word from \n",
    "# the dictionary we created earlier as \"ban_index_lookup\"\n",
    "print(sampled_token)\n",
    "decoded_sentence += \" \" + sampled_token     # Here, we cancatenated the retrived word with the decoded sentence \"[start]\"\n",
    "print(decoded_sentence);\n",
    "\n",
    "print(\"\\nNext word prediction...\\n\")\n",
    "# Below, we did the same thing to predict the next word depending on the \"start\" token and the word predicted earlier.\n",
    "tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "print(tokenized_target_sentence)\n",
    "next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
    "print(next_token_predictions[0, 1, :].shape)\n",
    "print(next_token_predictions[0, 1, :])\n",
    "sampled_token_index = np.argmax(next_token_predictions[0, 1, :]);\n",
    "print(sampled_token_index)\n",
    "sampled_token = ban_index_lookup[sampled_token_index]\n",
    "print(sampled_token)\n",
    "decoded_sentence += \" \" + sampled_token\n",
    "print(decoded_sentence);\n",
    "\n",
    "print(\"\\nNext word prediction...\\n\")\n",
    "# We can do it again and again ...\n",
    "tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "print(tokenized_target_sentence)\n",
    "next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
    "print(next_token_predictions[0, 1, :].shape)\n",
    "print(next_token_predictions[0, 1, :])\n",
    "sampled_token_index = np.argmax(next_token_predictions[0, 2, :]);\n",
    "print(sampled_token_index)\n",
    "sampled_token = ban_index_lookup[sampled_token_index]\n",
    "print(sampled_token)\n",
    "decoded_sentence += \" \" + sampled_token\n",
    "print(decoded_sentence);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a decode_sequence function\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = ban_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\": break\n",
    "    return(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "About seventy percentage of people are literate.\n",
      "[start] [UNK] [UNK] [UNK] [UNK] [UNK]               \n",
      "---\n",
      "Bangla is my mother tounge.\n",
      "[start] [UNK] [UNK] [UNK] [UNK]                \n",
      "---\n",
      "Bangladesh is a south-asian country.\n",
      "[start] [UNK] [UNK] [UNK] [UNK]                \n",
      "---\n",
      "I do not know him.\n",
      "[start] [UNK] [UNK] [UNK] [UNK]                \n"
     ]
    }
   ],
   "source": [
    "# Lets test the decoder\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(len(test_eng_texts)):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"---\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24d7359adec4ffe2916680474ceb48a86338759ffb8252cd67d6683f84078a4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
