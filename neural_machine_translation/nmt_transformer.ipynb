{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_file = \"eng-ban.txt\"\n",
    "text_file = \"ban-eng.txt\"\n",
    "# To open the text file we need to encode the text. Here, we use 'utf8' encoding\n",
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    bangla, english = line.split(\" >>> \")\n",
    "    bangla = \"[start] \" + bangla + \" [end]\"\n",
    "    text_pairs.append((english, bangla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Sentences:\", len(text_pairs))\n",
    "#text_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nope.', '[start] না. [end]')\n",
      "('About hundrad million people speaks in Bangla.', '[start] প্রায় এক কোটি মানুষ বাংলায় কথা বলে. [end]')\n",
      "('Sorry.', '[start] দুঃখিত. [end]')\n",
      "('Bangladesh is a developing country.', '[start] বাংলাদেশ একটি উন্নয়নশীল দেশ. [end]')\n",
      "('It is a Bangla to English translation file.', '[start] এটা বাংলা থেকে ইংরেজি অনুবাদের নথি. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    print(random.choice(text_pairs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "total_data_length = len(text_pairs)\n",
    "num_val_samples = int(0.20 * total_data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(total_data_length)\n",
    "print(num_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = (total_data_length - 2 * num_val_samples)\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Pairs: 12\n",
      "Number of Val Pairs: 4\n",
      "Number of Test Pairs: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Train Pairs: {len(train_pairs)}\")\n",
    "print(f\"Number of Val Pairs: {len(val_pairs)}\")\n",
    "print(f\"Number of Test Pairs: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation + \"\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"[{re.escape(strip_chars)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return(tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\"\n",
    "    ))\n",
    "\n",
    "vocab_size = 10000\n",
    "sequence_length = 20\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length + 1,\n",
    "    standardize = custom_standardization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_bangla_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_bangla_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, ban):\n",
    "    eng = source_vectorization(eng)\n",
    "    ban = source_vectorization(ban)\n",
    "    return({\"english\": eng, \"bangla\": ban[:, :-1]}, ban[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, ban_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    ban_texts = list(ban_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ban_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls = 4)\n",
    "    return(dataset.shuffle(2048).prefetch(16).cache())\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(train_ds.as_numpy_iterator()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (12, 20)\n",
      "inputs['bangla'].shape: (12, 19)\n",
      "target.shape: (12, 19)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['bangla'].shape: {inputs['bangla'].shape}\")\n",
    "    print(f\"target.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, None, 1024), dtype=tf.float32, name=None), name='gru_2/PartitionedCall:1', description=\"created by layer 'gru_2'\")\n"
     ]
    }
   ],
   "source": [
    "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"bangla\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 24s 24s/step - loss: 2.7861 - accuracy: 0.0000e+00 - val_loss: 2.6736 - val_accuracy: 0.8261\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.6672 - accuracy: 0.8261 - val_loss: 1.4697 - val_accuracy: 0.8261\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.3229 - accuracy: 0.8261 - val_loss: 0.3594 - val_accuracy: 0.8261\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3502 - accuracy: 0.8261 - val_loss: 0.6707 - val_accuracy: 0.2609\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6368 - accuracy: 0.3188 - val_loss: 0.3906 - val_accuracy: 0.8261\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3333 - accuracy: 0.8261 - val_loss: 0.3044 - val_accuracy: 0.8261\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2346 - accuracy: 0.8261 - val_loss: 0.2777 - val_accuracy: 0.8261\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2056 - accuracy: 0.8406 - val_loss: 0.2696 - val_accuracy: 0.8261\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1818 - accuracy: 0.8261 - val_loss: 0.2616 - val_accuracy: 0.8261\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1791 - accuracy: 0.8261 - val_loss: 0.2544 - val_accuracy: 0.8261\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1563 - accuracy: 0.8261 - val_loss: 0.2381 - val_accuracy: 0.8261\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1358 - accuracy: 0.8551 - val_loss: 0.2974 - val_accuracy: 0.8261\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1623 - accuracy: 0.8261 - val_loss: 0.3157 - val_accuracy: 0.5217\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2298 - accuracy: 0.5797 - val_loss: 0.3267 - val_accuracy: 0.8261\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1993 - accuracy: 0.8261 - val_loss: 0.2332 - val_accuracy: 0.8261\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1208 - accuracy: 0.8261 - val_loss: 0.2216 - val_accuracy: 0.8261\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1159 - accuracy: 0.8406 - val_loss: 0.2170 - val_accuracy: 0.8261\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1064 - accuracy: 0.8261 - val_loss: 0.1990 - val_accuracy: 0.8261\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0948 - accuracy: 0.8406 - val_loss: 0.3629 - val_accuracy: 0.8261\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1909 - accuracy: 0.8261 - val_loss: 0.3609 - val_accuracy: 0.5217\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2943 - accuracy: 0.4348 - val_loss: 0.2917 - val_accuracy: 0.8261\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1728 - accuracy: 0.8261 - val_loss: 0.2214 - val_accuracy: 0.8261\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1105 - accuracy: 0.8261 - val_loss: 0.2095 - val_accuracy: 0.8261\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1006 - accuracy: 0.8261 - val_loss: 0.2084 - val_accuracy: 0.8261\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0995 - accuracy: 0.8116 - val_loss: 0.1953 - val_accuracy: 0.8261\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0851 - accuracy: 0.8406 - val_loss: 0.2074 - val_accuracy: 0.8261\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0960 - accuracy: 0.8261 - val_loss: 0.1879 - val_accuracy: 0.9130\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0891 - accuracy: 0.8986 - val_loss: 0.3240 - val_accuracy: 0.8261\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.1605 - accuracy: 0.8261 - val_loss: 0.2549 - val_accuracy: 0.6522\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1844 - accuracy: 0.6377 - val_loss: 0.2670 - val_accuracy: 0.8261\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.1462 - accuracy: 0.8261 - val_loss: 0.1971 - val_accuracy: 0.8261\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0958 - accuracy: 0.8116 - val_loss: 0.1932 - val_accuracy: 0.8261\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0781 - accuracy: 0.8551 - val_loss: 0.1848 - val_accuracy: 0.8261\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0875 - accuracy: 0.8551 - val_loss: 0.1935 - val_accuracy: 0.8261\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0694 - accuracy: 0.8696 - val_loss: 0.1648 - val_accuracy: 0.9130\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0677 - accuracy: 0.9130 - val_loss: 0.4051 - val_accuracy: 0.8261\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.2210 - accuracy: 0.8261 - val_loss: 0.2691 - val_accuracy: 0.6957\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.2161 - accuracy: 0.6377 - val_loss: 0.3126 - val_accuracy: 0.8261\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.1841 - accuracy: 0.8261 - val_loss: 0.1937 - val_accuracy: 0.8261\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0924 - accuracy: 0.8551 - val_loss: 0.1823 - val_accuracy: 0.8261\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0870 - accuracy: 0.8841 - val_loss: 0.1981 - val_accuracy: 0.8261\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0794 - accuracy: 0.8696 - val_loss: 0.1718 - val_accuracy: 0.9565\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0903 - accuracy: 0.8406 - val_loss: 0.2599 - val_accuracy: 0.8261\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1146 - accuracy: 0.8261 - val_loss: 0.2158 - val_accuracy: 0.7826\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1457 - accuracy: 0.7101 - val_loss: 0.2775 - val_accuracy: 0.8261\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1437 - accuracy: 0.8261 - val_loss: 0.1807 - val_accuracy: 0.8696\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0831 - accuracy: 0.9130 - val_loss: 0.2017 - val_accuracy: 0.8261\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0762 - accuracy: 0.8551 - val_loss: 0.1687 - val_accuracy: 0.9130\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0739 - accuracy: 0.9130 - val_loss: 0.2255 - val_accuracy: 0.8261\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0703 - accuracy: 0.8696 - val_loss: 0.2064 - val_accuracy: 0.7826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x212aed3fc40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer = \"rmsprop\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "seq2seq_rnn.fit(train_ds, epochs=50, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ban_vocab = target_vectorization.get_vocabulary()\n",
    "ban_index_lookup = dict(zip(range(len(ban_vocab)), ban_vocab))\n",
    "max_decoded_sentence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eng Text: 12\n",
      "Test Eng Text: 4\n",
      "Val Eng Text: 4\n",
      "\n",
      "Train Ban Text: 12\n",
      "Test Ban Text: 4\n",
      "Val Ban Text: 4\n"
     ]
    }
   ],
   "source": [
    "# English texts\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "print(f\"Train Eng Text: {len(train_eng_texts)}\")\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "print(f\"Test Eng Text: {len(test_eng_texts)}\")\n",
    "\n",
    "val_eng_texts = [pair[0] for pair in val_pairs]\n",
    "print(f\"Val Eng Text: {len(val_eng_texts)}\")\n",
    "\n",
    "# Bangla texts\n",
    "train_ban_texts = [pair[1] for pair in train_pairs]\n",
    "print(f\"\\nTrain Ban Text: {len(train_ban_texts)}\")\n",
    "\n",
    "test_ban_texts = [pair[1] for pair in test_pairs]\n",
    "print(f\"Test Ban Text: {len(test_ban_texts)}\")\n",
    "\n",
    "val_ban_texts = [pair[1] for pair in val_pairs]\n",
    "print(f\"Val Ban Text: {len(val_ban_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bangla is a language.\n",
      "tf.Tensor([[2 3 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "input_sentence = random.choice(test_eng_texts)\n",
    "print(input_sentence)\n",
    "tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "print(tokenized_input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not know him.\n",
      "[start] আমি উনাকে চিনি না. [end]\n",
      "tf.Tensor([[7 1 1 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 20), dtype=int64)\n",
      "tf.Tensor([[1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "input_sentence = random.choice(test_pairs)\n",
    "print(input_sentence[0])\n",
    "print(input_sentence[1])\n",
    "tokenized_input_sentence0 = source_vectorization([input_sentence[0]])\n",
    "print(tokenized_input_sentence0)\n",
    "tokenized_input_sentence1 = source_vectorization([input_sentence[1]])\n",
    "print(tokenized_input_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(21,), dtype=int64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 1, 21\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_604/1331219510.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenized_target_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_vectorization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_target_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnext_token_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq2seq_rnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenized_input_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenized_target_sentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_token_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_token_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AbsSayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AbsSayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[0;32m   1656\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1, 21\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "decoded_sentence = \"[start]\"\n",
    "tokenized_target_sentence = target_vectorization(decoded_sentence)\n",
    "print(tokenized_target_sentence)\n",
    "next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
    "print(next_token_predictions[0, 0, :].shape)\n",
    "print(next_token_predictions[0, 0, :])\n",
    "sampled_token_index = np.argmax(next_token_predictions[0, 0, :]);\n",
    "print(sampled_token_index)\n",
    "sampled_token = ban_index_lookup[sampled_token_index]\n",
    "print(sampled_token)\n",
    "decoded_sentence += \" \" + sampled_token\n",
    "print(decoded_sentence);\n",
    "\n",
    "tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "print(tokenized_target_sentence)\n",
    "next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
    "print(next_token_predictions[0, 1, :].shape)\n",
    "print(next_token_predictions[0, 1, :])\n",
    "sampled_token_index = np.argmax(next_token_predictions[0, 1, :]);\n",
    "print(sampled_token_index)\n",
    "sampled_token = ban_index_lookup[sampled_token_index]\n",
    "print(sampled_token)\n",
    "decoded_sentence += \" \" + sampled_token\n",
    "print(decoded_sentence);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24d7359adec4ffe2916680474ceb48a86338759ffb8252cd67d6683f84078a4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
